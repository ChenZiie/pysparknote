{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD:\n",
    "\n",
    "Transformations:\n",
    "    map(f:T->U)                                :RDD[T]->RDD[U]\n",
    "    filter(f:T->Bool)                          :RDD[T]->RDD[T]\n",
    "    flatMap(f:T->Seq[U])                       :RDD[T]->RDD[U]\n",
    "    sample(Withreplacemen:Bool,fration:Float)  :RDD[T]->RDD[T]                 (sampling)\n",
    "    groupByKey()                               :RDD[(K,V)]->RDD[(K,Seq[V])]    (same key will be group together)\n",
    "    reduceByKey(f:(V,V)->V))                   :RDD[(K,V)]->RDD[(K,V)]\n",
    "    union()                                    :(RDD[T],RDD[T])->RDD[T]\n",
    "    join()                                     :(RDD[(K,V)],RDD[(K,V)])->RDD[(K,(V,W))]\n",
    "    cogroup()                                  :(RDD[(K,V)],RDD[(K,W)])->RDD[(K,(Seq(V),Seq(W))]\n",
    "    crossProduct()                             :(RDD[T],RDD[U])->RDD[(T,U)]\n",
    "    mapValues(f:V->W)                          :RDD[(K,V)]->RDD[(K,W)]          (Preserves partitioning) \n",
    "    sortBy(f:V)                                :RDD[(K,V)]->RDD[(K,V)]\n",
    "    sortBysortBykey()                          :RDD[(K,V)]->RDD[(K,V)] \n",
    "    partitionBy(p:Partitioner[K])              :RDD[(K,V)]->RDD[(K,V)]\n",
    "    mapPartitions(f)                           :Do actions inside each partitions\n",
    "    glom()                                     :show the elements inside each partitions\n",
    "    mapPartitionsWithIndex(f:index,list->somrthing):\n",
    "    \n",
    "   \n",
    "Actions:\n",
    "    count()                                    :RDD[T]->Long\n",
    "    collect()                                  :RDD[T]->Seq[T]\n",
    "    reduce(f:(T,T)->T)                         :RDD[T]->T          \n",
    "    lookup(k:K)                                :RDD[(K,V)]->Seq[V] (on hash/range partitioned RDDs)    \n",
    "    save(path:String)                          :outputs RDD to a storage system, e.g.,HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry', 'strawberry']\n",
      "apple\n",
      "['apple', 'banana', 'banana']\n"
     ]
    }
   ],
   "source": [
    "#read textFile with 4 partitions\n",
    "fruits = sc.textFile('fruits.txt',4)\n",
    "print(fruits.collect())\n",
    "#cache()\n",
    "fruits.cache()\n",
    "fruits.unpersist()\n",
    "print(fruits.first())\n",
    "print(fruits.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = sc.parallelize(range(1, 10))    #creat a rdd\n",
    "A.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10)]\n",
      "[(1, 12), (2, 13), (3, 14), (4, 15), (5, 16), (6, 17), (7, 18), (8, 19), (9, 20)]\n",
      "[1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "#map\n",
    "B = A.map(lambda x: (x,x+1))\n",
    "print(B.collect())\n",
    "#mapValues\n",
    "print(B.mapValues(lambda x:x+10).collect())\n",
    "#flatMap\n",
    "C = B.flatMap(lambda x: list(x))\n",
    "print(C.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter\n",
    "t = 5\n",
    "A.filter(lambda x: x < t).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 8]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample\n",
    "C.sample(False,0.3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (1, 6), (1, 7), (1, 8), (1, 9)]\n",
      "2\n",
      "[(1, 9), (2, 5)]\n"
     ]
    }
   ],
   "source": [
    "#groupByKey\n",
    "B = A.map(lambda x: (1,x) if x>5 else (2,x))\n",
    "print(B.collect())\n",
    "print(B.groupByKey().count())\n",
    "#reduceByKey\n",
    "print(B.reduceByKey(lambda a,b:a if a>b else b).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10]\n",
      "join\n",
      "[(2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (1, 6), (1, 7), (1, 8), (1, 9)]\n",
      "[(1, 1), (1, 2), (1, 3), (1, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9)]\n",
      "[(1, (6, 1)), (1, (6, 2)), (1, (6, 3)), (1, (6, 4)), (1, (7, 1)), (1, (7, 2)), (1, (7, 3)), (1, (7, 4)), (1, (8, 1)), (1, (8, 2)), (1, (8, 3)), (1, (8, 4)), (1, (9, 1)), (1, (9, 2)), (1, (9, 3)), (1, (9, 4)), (2, (1, 5)), (2, (1, 6)), (2, (1, 7)), (2, (1, 8)), (2, (1, 9)), (2, (2, 5)), (2, (2, 6)), (2, (2, 7)), (2, (2, 8)), (2, (2, 9)), (2, (3, 5)), (2, (3, 6)), (2, (3, 7)), (2, (3, 8)), (2, (3, 9)), (2, (4, 5)), (2, (4, 6)), (2, (4, 7)), (2, (4, 8)), (2, (4, 9)), (2, (5, 5)), (2, (5, 6)), (2, (5, 7)), (2, (5, 8)), (2, (5, 9))]\n"
     ]
    }
   ],
   "source": [
    "#union() \n",
    "print(A.collect())\n",
    "print(C.collect())\n",
    "print(A.union(C).collect())\n",
    "#join() \n",
    "print('join')\n",
    "print(B.collect())\n",
    "D = A.map(lambda x: (1,x) if x<5 else (2,x))\n",
    "print(D.collect())\n",
    "print(B.join(D).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[(1, 6), (1, 7), (1, 8), (1, 9), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5)]\n"
     ]
    }
   ],
   "source": [
    "#sortBysortBy()\n",
    "print(A.sortBy(lambda x:x).collect())\n",
    "#sortBysortBykey()\n",
    "print(B.sortByKey().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL:\n",
    "SQL:\n",
    "SQL:\n",
    "SQL:\n",
    "SQL:\n",
    "SQL:\n",
    "SQL:\n",
    "More detail please go to the notebook(sparksql.ipynb) which Dr.Yi provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up\n",
    "from pyspark.sql.functions import *\n",
    "df = spark.read.csv('building.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "|         1|         M1|         25|     AC1000|         USA|\n",
      "|         2|         M2|         27|     FN39TG|      France|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|\n",
      "|         4|         M4|         17|     GG1919|     Finland|\n",
      "|         5|         M5|          3|    ACMAX22|   Hong Kong|\n",
      "|         6|         M6|          9|     AC1000|   Singapore|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|\n",
      "|        10|        M10|         23|    ACMAX22|       China|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|\n",
      "|        14|        M14|         17|     GG1919|     Germany|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|\n",
      "|        19|        M19|         14|     GG1919|      Canada|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the content of the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BuildingID: integer (nullable = true)\n",
      " |-- BuildingMgr: string (nullable = true)\n",
      " |-- BuildingAge: integer (nullable = true)\n",
      " |-- HVACproduct: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the dataframe schema in a tree format\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(BuildingID=1, BuildingMgr='M1', BuildingAge=25, HVACproduct='AC1000', Country='USA'),\n",
       " Row(BuildingID=2, BuildingMgr='M2', BuildingAge=27, HVACproduct='FN39TG', Country='France'),\n",
       " Row(BuildingID=3, BuildingMgr='M3', BuildingAge=28, HVACproduct='JDNS77', Country='Brazil')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an RDD from the dataframe\n",
    "dfrdd = df.rdd\n",
    "dfrdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|BuildingID|     Country|\n",
      "+----------+------------+\n",
      "|         1|         USA|\n",
      "|         2|      France|\n",
      "|         3|      Brazil|\n",
      "|         4|     Finland|\n",
      "|         5|   Hong Kong|\n",
      "|         6|   Singapore|\n",
      "|         7|South Africa|\n",
      "|         8|   Australia|\n",
      "|         9|      Mexico|\n",
      "|        10|       China|\n",
      "|        11|     Belgium|\n",
      "|        12|     Finland|\n",
      "|        13|Saudi Arabia|\n",
      "|        14|     Germany|\n",
      "|        15|      Israel|\n",
      "|        16|      Turkey|\n",
      "|        17|       Egypt|\n",
      "|        18|   Indonesia|\n",
      "|        19|      Canada|\n",
      "|        20|   Argentina|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve specific columns from the dataframe\n",
    "df.select('BuildingID', 'Country').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|BuildingID|new colum|\n",
      "+----------+---------+\n",
      "|         2|        4|\n",
      "|         3|        6|\n",
      "|         4|        8|\n",
      "|         5|       10|\n",
      "|         6|       12|\n",
      "|         7|       14|\n",
      "|         8|       16|\n",
      "|         9|       18|\n",
      "|        10|       20|\n",
      "|        11|       22|\n",
      "|        12|       24|\n",
      "|        13|       26|\n",
      "|        14|       28|\n",
      "|        15|       30|\n",
      "|        16|       32|\n",
      "|        17|       34|\n",
      "|        18|       36|\n",
      "|        19|       38|\n",
      "|        20|       40|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.where(\"Country<'USA'\").select('BuildingID', lit(df['BuildingID']*2).alias('new colum'))\n",
    "#where can be replace by filter                create new column and give a name\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+-------+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|Country|\n",
      "+----------+-----------+-----------+-----------+-------+\n",
      "|         3|         M3|         28|     JDNS77| Brazil|\n",
      "|         2|         M2|         27|     FN39TG| France|\n",
      "|         1|         M1|         25|     AC1000|    USA|\n",
      "+----------+-----------+-----------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#orderBy()\n",
    "df.where(\"BuildingID<='3'\").orderBy('BuildingAge',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|HVACProduct|count|\n",
      "+-----------+-----+\n",
      "|ACMAX22    |4    |\n",
      "|AC1000     |4    |\n",
      "|JDNS77     |4    |\n",
      "|FN39TG     |4    |\n",
      "|GG1919     |4    |\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use GroupBy clause with dataframe \n",
    "df.groupBy('HVACProduct').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "|         1|         M1|         25|     AC1000|         USA|\n",
      "|         2|         M2|         27|     FN39TG|      France|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|\n",
      "|         4|         M4|         17|     GG1919|     Finland|\n",
      "|         5|         M5|          3|    ACMAX22|   Hong Kong|\n",
      "|         6|         M6|          9|     AC1000|   Singapore|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|\n",
      "|        10|        M10|         23|    ACMAX22|       China|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|\n",
      "|        14|        M14|         17|     GG1919|     Germany|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|\n",
      "|        19|        M19|         14|     GG1919|      Canada|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "\n",
      "+----------+---------+\n",
      "|BuildingID|new colum|\n",
      "+----------+---------+\n",
      "|         2|        4|\n",
      "|         3|        6|\n",
      "|         4|        8|\n",
      "|         5|       10|\n",
      "|         6|       12|\n",
      "|         7|       14|\n",
      "|         8|       16|\n",
      "|         9|       18|\n",
      "|        10|       20|\n",
      "|        11|       22|\n",
      "|        12|       24|\n",
      "|        13|       26|\n",
      "|        14|       28|\n",
      "|        15|       30|\n",
      "|        16|       32|\n",
      "|        17|       34|\n",
      "|        18|       36|\n",
      "|        19|       38|\n",
      "|        20|       40|\n",
      "+----------+---------+\n",
      "\n",
      "+----------+-----------+-----------+-----------+------------+---+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country| my|\n",
      "+----------+-----------+-----------+-----------+------------+---+\n",
      "|         2|         M2|         27|     FN39TG|      France|  4|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|  6|\n",
      "|         4|         M4|         17|     GG1919|     Finland|  8|\n",
      "|         5|         M5|          3|    ACMAX22|   Hong Kong| 10|\n",
      "|         6|         M6|          9|     AC1000|   Singapore| 12|\n",
      "|         7|         M7|         13|     FN39TG|South Africa| 14|\n",
      "|         8|         M8|         25|     JDNS77|   Australia| 16|\n",
      "|         9|         M9|         11|     GG1919|      Mexico| 18|\n",
      "|        10|        M10|         23|    ACMAX22|       China| 20|\n",
      "|        11|        M11|         14|     AC1000|     Belgium| 22|\n",
      "|        12|        M12|         26|     FN39TG|     Finland| 24|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia| 26|\n",
      "|        14|        M14|         17|     GG1919|     Germany| 28|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel| 30|\n",
      "|        16|        M16|         23|     AC1000|      Turkey| 32|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt| 34|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia| 36|\n",
      "|        19|        M19|         14|     GG1919|      Canada| 38|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina| 40|\n",
      "+----------+-----------+-----------+-----------+------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#join and rename\n",
    "df.show()\n",
    "df1.show()\n",
    "df.join(df1,'BuildingID').withColumnRenamed('new colum', 'my').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sum(revenue)=1016198410.83)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#past exam example\n",
    "dfTransaction = spark.read.csv('Transaction.csv', header=True, inferSchema=True)\n",
    "dfProduct = spark.read.csv('Product2020.csv', header=True, inferSchema=True)\n",
    "\n",
    "#The following two values will change during the grading.  \n",
    "#For test purposes during the exam, you can set them to be\n",
    "RequiredBrand= \"Brand#23\"\n",
    "RequiredContainer = \"MED BOX\"\n",
    "\n",
    "# s = dfTransaction.groupBy('Product_ID').sum('SaleQuantity')\n",
    "# c = dfTransaction.groupBy('Product_ID').count()\n",
    "# print(c.show())\n",
    "select_condition = 'Brand = \\'' + RequiredBrand + '\\' and Container = \\'' + RequiredContainer + '\\''\n",
    "dfProductSelect = dfProduct.where(select_condition).select(\"Product_ID\").withColumnRenamed(\"Product_ID\", \"p_ID\")\n",
    "dfJoin = dfProductSelect.join(dfTransaction, dfProductSelect.p_ID == dfTransaction.Product_ID)\n",
    "dfAvg = dfJoin.groupBy(\"p_ID\").avg(\"SaleQuantity\").withColumnRenamed(\"avg(SaleQuantity)\", \"avgQuantity\")\n",
    "dfFinal = dfAvg.join(dfJoin, dfAvg.p_ID == dfJoin.p_ID).filter(\"avgQuantity > SaleQuantity\")\n",
    "dfFinal.select('*', lit(dfFinal.SalePrice*dfFinal.SaleQuantity).alias('revenue')).groupby().sum('revenue').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internal\n",
    "Internal\n",
    "Internal\n",
    "Internal\n",
    "Internal\n",
    "Internal\n",
    "Internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 4]\n",
      "[[(2, 1), (2, 2), (2, 3), (2, 4), (2, 5)], [(1, 6), (1, 7), (1, 8), (1, 9)]]\n",
      "[[(2, 1), (2, 2)], [(2, 3), (2, 4)], [(2, 5), (1, 6)], [(1, 7), (1, 8), (1, 9)]]\n"
     ]
    }
   ],
   "source": [
    "#partitionBy()\n",
    "#mapPartitions()\n",
    "#glom()\n",
    "#internal\n",
    "\n",
    "def partitionsize(it): \n",
    "    yield len(list(it))\n",
    "\n",
    "print(B.partitionBy(2,lambda x:x%2).mapPartitions(partitionsize).collect())  #size of each patition\n",
    "print(B.partitionBy(2,lambda x:x%2).glom().collect())        #element of each partition\n",
    "print(B.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm\n",
    "Algorithm\n",
    "Algorithm\n",
    "Algorithm\n",
    "Algorithm\n",
    "Algorithm\n",
    "Algorithm\n",
    "Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('a', 'a'), ('b', 'b'), ('c', 'c'), ('c', 'c'), ('c', 'c')], [('b', 'b'), ('c', 'c'), ('b', 'c'), ('c', 'c'), ('a', 'a')], [('c', 'c'), ('a', 'a'), ('c', 'c'), ('c', 'c'), ('a', 'a')], [('c', 'c'), ('a', 'a'), ('a', 'a'), ('b', 'b'), ('b', 'b')]]\n",
      "['<']\n",
      "<\n"
     ]
    }
   ],
   "source": [
    "#used mapPartitions() \n",
    "x = 'abcccbcbcacaccacaabb'\n",
    "y = 'abcccbcccacaccacaabb'\n",
    "numPartitions = 4\n",
    "rdd = sc.parallelize(zip(x,y), numPartitions)\n",
    "print(rdd.glom().collect())\n",
    "\n",
    "def f(i):\n",
    "    for x in i:\n",
    "        if x[0] > x[1]:\n",
    "            yield '>'\n",
    "            break\n",
    "        elif x[0] < x[1]:\n",
    "            yield '<'\n",
    "            break\n",
    "\n",
    "results = rdd.mapPartitions(f).collect()\n",
    "print(results)\n",
    "if len(results) == 0:\n",
    "    print('=')\n",
    "else:\n",
    "    print(results[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph:\n",
    "Graph:\n",
    "Graph:\n",
    "Graph:\n",
    "Graph:\n",
    "Graph:\n",
    "Graph:\n",
    "Graph:\n",
    "Graph:\n",
    "Graph:\n",
    "./hadoop/spark-3.0.3/bin/pyspark --packages graphframes:graphframes:0.8.1-spark3.0-s_2.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile(\"../graphframes-0.8.1-spark3.0-s_2.12.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GraphFrame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-eccf3cb921e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Create a GraphFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGraphFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GraphFrame' is not defined"
     ]
    }
   ],
   "source": [
    "# Vertics DataFrame\n",
    "v = spark.createDataFrame([\n",
    "  (\"a\", \"Alice\", 34),\n",
    "  (\"b\", \"Bob\", 36),\n",
    "  (\"c\", \"Charlie\", 37),\n",
    "  (\"d\", \"David\", 29),\n",
    "  (\"e\", \"Esther\", 32),\n",
    "  (\"f\", \"Fanny\", 38),\n",
    "  (\"g\", \"Gabby\", 60)\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Edges DataFrame\n",
    "e = spark.createDataFrame([\n",
    "  (\"a\", \"b\", \"friend\"),\n",
    "  (\"b\", \"c\", \"follow\"),\n",
    "  (\"c\", \"b\", \"follow\"),\n",
    "  (\"f\", \"c\", \"follow\"),\n",
    "  (\"e\", \"f\", \"follow\"),\n",
    "  (\"e\", \"d\", \"friend\"),\n",
    "  (\"d\", \"a\", \"friend\"),\n",
    "  (\"a\", \"e\", \"friend\"),\n",
    "  (\"g\", \"e\", \"follow\")\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "\n",
    "# Create a GraphFrame\n",
    "g = GraphFrame(v, e)\n",
    "\n",
    "g.vertices.show()\n",
    "g.edges.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stream:\n",
    "Stream:\n",
    "Stream:\n",
    "Stream:\n",
    "Stream:\n",
    "Stream:\n",
    "Stream:\n",
    "Stream:\n",
    "Stream:\n",
    "updateStateByKey(updateFunc) :keep the last time values, and the current input will be a list of somthing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('social', 'constructionism'), ('coercive', 'deprogramming'), ('individual', 'meaninglessness'), ('political', 'self-determination'), ('desirable', 'characteristic')]\n",
      "GOOD: ['morning/afternoon']\n",
      "\n",
      "[('social', 'constructionism'), ('coercive', 'deprogramming'), ('individual', 'meaninglessness'), ('political', 'self-determination'), ('desirable', 'characteristic')]\n",
      "GOOD: ['morning/afternoon']\n",
      "\n",
      "[('social', 'constructionism'), ('coercive', 'administration'), ('individual', 'meaninglessness'), ('political', 'assassinations-were'), ('desirable', 'characteristic')]\n",
      "GOOD: ['morning/afternoon']\n",
      "\n",
      "[('social', 'constructionism'), ('coercive', 'administration'), ('individual', 'being-in-the-world'), ('political', 'assassinations-were'), ('desirable', 'characteristic')]\n",
      "GOOD: ['morning/afternoon']\n",
      "\n",
      "[('social', 'realism/constructivism'), ('coercive', 'administration'), ('individual', 'being-in-the-world'), ('political', 'assassinations-were'), ('desirable', 'characteristic')]\n",
      "GOOD: ['morning/afternoon']\n",
      "\n",
      "[('social', 'realism/constructivism'), ('coercive', 'administration'), ('individual', 'being-in-the-world'), ('political', 'assassinations-were'), ('desirable', 'characteristic')]\n",
      "GOOD: ['morning/afternoon']\n",
      "\n",
      "[('social', 'realism/constructivism'), ('coercive', 'administration'), ('individual', 'being-in-the-world'), ('political', 'assassinations-were'), ('desirable', 'characteristic')]\n",
      "GOOD: ['morning/afternoon']\n",
      "\n",
      "[('social', 'realism/constructivism'), ('coercive', 'administration'), ('individual', 'being-in-the-world'), ('political', 'assassinations-were'), ('desirable', 'characteristic')]\n",
      "GOOD: ['morning/afternoon']\n",
      "\n",
      "[('social', 'realism/constructivism'), ('coercive', 'self-incrimination'), ('individual', 'being-in-the-world'), ('political', 'assassinations-were'), ('desirable', 'characteristic')]\n",
      "GOOD: ['morning/afternoon']\n",
      "\n",
      "[('social', 'realism/constructivism'), ('coercive', 'self-incrimination'), ('individual', 'being-in-the-world'), ('political', 'assassinations-were'), ('desirable', 'characteristic')]\n",
      "GOOD: ['morning/afternoon']\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#Q5\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# Provide a checkpointing directory. Required for stateful transformations\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "numPartitions = 8\n",
    "rdd = sc.textFile('adj_noun_pairs.txt', numPartitions)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123)\n",
    "\n",
    "\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "def updateFunc(newValues, running):\n",
    "    if running == None and len(newValues) != 0:\n",
    "        return newValues[0]\n",
    "    elif running != None and len(newValues) == 0:\n",
    "        return running\n",
    "    elif running != None and len(newValues) != 0:\n",
    "        if len(newValues[0]) > len(running):\n",
    "            return newValues[0]\n",
    "        else:\n",
    "            return running\n",
    "\n",
    "def printre(rdd):\n",
    "    print(rdd.take(5))\n",
    "    \n",
    "def printgood(rdd):\n",
    "    print(\"GOOD:\",rdd.map(lambda rdd: rdd[1]).take(1))\n",
    "    print()\n",
    "\n",
    "pairs  = lines.flatMap(lambda line: [line.split(\" \")]).filter(lambda x: len(x) == 2)\\\n",
    "        .map(lambda line: (line[0],line[1]))\\\n",
    "        .reduceByKey(lambda x,y:x if len(x) >= len(y) else y)\\\n",
    "        .updateStateByKey(updateFunc)\n",
    "pairs.foreachRDD(printre)\n",
    "\n",
    "good = pairs.filter(lambda x: x[0] == 'good')\n",
    "good.foreachRDD(printgood)\n",
    "\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(50)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 4\n",
    "rdd = sc.parallelize([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,1,1,1,1,1,1], p)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, False], [3, 0, 3, False], [3, 0, 3, False], [0, 6, 6, False]]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "def divide(it):\n",
    "    left_one = 0    # Number of continuous 1's counted from left\n",
    "    right_one = 0    # Number of continuous 1's counted from right\n",
    "    local_one = 0    # The longest continuous 1's within partition\n",
    "    indicator = True # indicates whether the partition is all 1's\n",
    "    running_one = 0\n",
    "    \n",
    "    for x in it:\n",
    "        if (x == 0):\n",
    "            if (indicator): # Encountered the first zero\n",
    "                left_one = running_one \n",
    "            if (running_one > local_one):\n",
    "                local_one = running_one\n",
    "            running_one = 0\n",
    "            indicator = False\n",
    "        else:\n",
    "            running_one += 1\n",
    "    if local_one < running_one:\n",
    "        local_one = running_one\n",
    "    right_one = running_one\n",
    "    if (indicator): # update the case of all 1's\n",
    "        left_one = running_one\n",
    "        local_one = running_one\n",
    "            \n",
    "    yield [left_one, right_one, local_one, indicator]\n",
    "    \n",
    "    \n",
    "L = rdd.mapPartitions(divide).collect()\n",
    "print(L)\n",
    "def conquer(L):\n",
    "    res = 0\n",
    "    running_one = 0\n",
    "    \n",
    "    for x in L:\n",
    "        (left_one, right_one, local_one, indicator) = x\n",
    "        # case 1, maximum appears within site\n",
    "        if (local_one > res):\n",
    "            res = local_one\n",
    "        # case 2 and 3, maximum cross multiple partitions\n",
    "        if (indicator): # carry to the next partition\n",
    "            running_one += local_one\n",
    "        else: # solve the cross-partition max\n",
    "            running_one += left_one\n",
    "            if (running_one > res):\n",
    "                res = running_one\n",
    "            running_one = right_one\n",
    "    # deal with the end\n",
    "#     if (running_one > res):\n",
    "#         res = running_one\n",
    "    return res\n",
    "            \n",
    "print(conquer(L))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 0, 1], [1, 1, 1, 0], [1, 1, 1, 0], [0, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Past Exam Examples\n",
    "Past Exam Examples\n",
    "Past Exam Examples\n",
    "Past Exam Examples\n",
    "Past Exam Examples\n",
    "Past Exam Examples\n",
    "Past Exam Examples\n",
    "Past Exam Examples\n",
    "Past Exam Examples\n",
    "Past Exam Examples\n",
    "Past Exam Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9)]\n",
      "285\n"
     ]
    }
   ],
   "source": [
    "#2020 part2 example2\n",
    "RDD1 = sc.parallelize(range(1, 10))\n",
    "RDD2 = sc.parallelize(range(1, 10))\n",
    "r1 = RDD1.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "print(r1.collect())\n",
    "r2 = RDD2.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "RDD3 = r1.union(r2).reduceByKey(lambda x,y: x*y).map(lambda x: x[1])\n",
    "dot_product = RDD3.reduce(lambda x,y: x+y)\n",
    "print(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('external', 'link'), 836), (('19th', 'century'), 327), (('same', 'time'), 280), (('20th', 'century'), 266), (('first', 'time'), 264), (('other', 'hand'), 236), (('large', 'number'), 227), (('civil', 'war'), 211), (('political', 'party'), 201), (('recent', 'year'), 189)]\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#2020 part2 example 3\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# Provide a checkpointing directory.  Required for stateful transformations\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "rdd = sc.textFile('adj_noun_pairs.txt', 8).map(lambda l: tuple(l.split())).filter(lambda p: len(p)==2)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "def updateFunc(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)\n",
    "\n",
    "running_counts = lines.map(lambda x: (x,1))\\\n",
    "                      .updateStateByKey(updateFunc)\n",
    "\n",
    "counts_sorted = running_counts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "                            #in stream need to use transform to do rdd functions\n",
    "\n",
    "# counts_sorted = running_counts.map(lambda x: (x[0][1], x)). \\\n",
    "#                                reduceByKey(lambda x,y: x if x[1] > y[1] else y). \\\n",
    "#                                map(lambda x: x[1]). \\\n",
    "#                                transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "\n",
    "def printResults(rdd):\n",
    "    print(rdd.take(10))\n",
    "\n",
    "counts_sorted.foreachRDD(printResults)\n",
    "\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(5)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 4, 6, 5]\n"
     ]
    }
   ],
   "source": [
    "#2019 F Q4\n",
    "rdd1 = sc.parallelize([5,0,0,3])\n",
    "rdd2 = sc.parallelize([3,4,6,2])\n",
    "\n",
    "def ele_wise_add(rdd1, rdd2): \n",
    "    rdd1 = rdd1.zipWithIndex().map(lambda x:(x[1],x[0]))\n",
    "    rdd2 = rdd2.zipWithIndex().map(lambda x:(x[1],x[0]))\n",
    "    return rdd1.union(rdd2).reduceByKey(lambda x,y:x+y).map(lambda x:x[1])\n",
    "\n",
    "rdd3 = ele_wise_add(rdd1, rdd2)\n",
    "print(rdd3.collect())\n",
    "# Expected output: [8,4,6,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  x|  y|  z|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e=sc.parallelize([(1,2),(1,3),(1,4),(2,3),(3,1)]).toDF([\"src\",\"dst\"])\n",
    "\n",
    "e1=e.withColumnRenamed('src', 'x').withColumnRenamed('dst','y')\n",
    "e2=e.withColumnRenamed('src', 'y1').withColumnRenamed('dst','z')\n",
    "e3=e.withColumnRenamed('src', 'z1').withColumnRenamed('dst','x1')\n",
    "result = e1.join(e2).join(e3).where(\"y = y1 AND z = z1 AND x = x1 AND x < y AND y < z\")\n",
    "result.select('x','y','z').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|order_count|cust_count|\n",
      "+-----------+----------+\n",
      "|          0|       500|\n",
      "|         11|        67|\n",
      "|         12|        63|\n",
      "|         10|        63|\n",
      "|          9|        63|\n",
      "|          8|        62|\n",
      "|         14|        57|\n",
      "|         20|        55|\n",
      "|         13|        50|\n",
      "|         15|        45|\n",
      "|         21|        44|\n",
      "|          7|        43|\n",
      "|         18|        42|\n",
      "|         16|        42|\n",
      "|         17|        40|\n",
      "|         24|        36|\n",
      "|         22|        36|\n",
      "|         19|        36|\n",
      "|          6|        32|\n",
      "|         23|        25|\n",
      "+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCustomer = spark.read.csv('customer1.csv', header=True, inferSchema=True)\n",
    "dfOrders = spark.read.csv('orders.csv', header=True, inferSchema=True)\n",
    "from pyspark.sql.functions import *\n",
    "dfOrdersGroup = dfOrders.groupBy(\"CUSTKEY\").count()\n",
    "dfOrdersGroup.createOrReplaceTempView(\"orders\")\n",
    "dfCustomer.createOrReplaceTempView(\"customer\")\n",
    "result = spark.sql(\"SELECT customer.CUSTKEY, count FROM customer left outer join orders on customer.CUSTKEY = orders.CUSTKEY\")\\\n",
    "        .na.fill(0)\\\n",
    "        .withColumnRenamed(\"count\", \"order_count\").groupBy(\"order_count\").count()\\\n",
    "        .withColumnRenamed(\"count\",\"cust_count\")\\\n",
    "        .sort(desc(\"cust_count\"),desc(\"order_count\"))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "acc = sc.accumulator(0)\n",
    "def f(x):\n",
    "     acc.add(x)\n",
    "# sc.parallelize([1,2,3,4]).foreach(lambda x:f(x))\n",
    "print(sc.parallelize([1,2,3,4]).map(lambda x:f(x)).collect())\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(99747, 35557),\n",
       " (93036, 99441),\n",
       " (98158, 92386),\n",
       " (99623, 73826),\n",
       " (99978, 19775),\n",
       " (99210, 88620),\n",
       " (41277, 99990),\n",
       " (94106, 99309),\n",
       " (99773, 34927),\n",
       " (95716, 98344),\n",
       " (99989, 21634),\n",
       " (99480, 95038),\n",
       " (98610, 95551),\n",
       " (99958, 59581),\n",
       " (95086, 99479),\n",
       " (99610, 63697),\n",
       " (92580, 99974),\n",
       " (96525, 96473),\n",
       " (99202, 47009),\n",
       " (92327, 99855),\n",
       " (95598, 93566),\n",
       " (99476, 46250),\n",
       " (98590, 88094),\n",
       " (98766, 75551),\n",
       " (94187, 99665),\n",
       " (99799, 8240),\n",
       " (95312, 95227),\n",
       " (97493, 88501),\n",
       " (26419, 99969),\n",
       " (8928, 99976),\n",
       " (80530, 99587),\n",
       " (98814, 98537),\n",
       " (99958, 26054),\n",
       " (99956, 79605),\n",
       " (99740, 98217),\n",
       " (88007, 98944),\n",
       " (41391, 99972),\n",
       " (78056, 99970),\n",
       " (59920, 99992),\n",
       " (98231, 94688),\n",
       " (99568, 91339),\n",
       " (96815, 99398),\n",
       " (99913, 68052),\n",
       " (96167, 99795),\n",
       " (51623, 99893),\n",
       " (95316, 98802),\n",
       " (98192, 94109),\n",
       " (96629, 98467),\n",
       " (99969, 11482),\n",
       " (90667, 99089),\n",
       " (94683, 99039),\n",
       " (79046, 99675),\n",
       " (59329, 99737),\n",
       " (99010, 77446),\n",
       " (99662, 75927),\n",
       " (98161, 84105),\n",
       " (99652, 83201),\n",
       " (87366, 98476),\n",
       " (45347, 99791),\n",
       " (82101, 99614),\n",
       " (99738, 62174),\n",
       " (87601, 97215),\n",
       " (97809, 96611),\n",
       " (99990, 53983),\n",
       " (97734, 78832),\n",
       " (99809, 66096),\n",
       " (54294, 99839),\n",
       " (93308, 99692),\n",
       " (98241, 74070),\n",
       " (97727, 95361),\n",
       " (92693, 99960),\n",
       " (99977, 59457),\n",
       " (96731, 99071),\n",
       " (17304, 99970),\n",
       " (99578, 96784),\n",
       " (93302, 99355),\n",
       " (99885, 83645),\n",
       " (99942, 21085),\n",
       " (87636, 99678),\n",
       " (62061, 99726),\n",
       " (99827, 89705),\n",
       " (99662, 98973),\n",
       " (45834, 100000)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2019 Q2\n",
    "numPartitions = 10\n",
    "\n",
    "points = sc.textFile('points.txt',numPartitions)\n",
    "pairs = points.map(lambda l: tuple(l.split()))\n",
    "pairs = pairs.map(lambda pair: (int(pair[0]),int(pair[1])))\n",
    "pairs.cache()\n",
    "\n",
    "def f(iterator):\n",
    "    dump = list(iterator)\n",
    "    for P in dump:\n",
    "        k = 1\n",
    "        for Q in dump:\n",
    "            if P[0] == Q[0] and P[1] == Q[1]:\n",
    "                continue\n",
    "            if P[0] <= Q[0] and P[1] <= Q[1]:\n",
    "                k = 0\n",
    "                break\n",
    "        if k == 1:\n",
    "            yield P\n",
    "\n",
    "temp = pairs.mapPartitions(f).collect()\n",
    "result = []\n",
    "for i in temp:\n",
    "    k = 1;\n",
    "    for j in temp:\n",
    "        if i[0] == j[0] and i[1] == j[1]:\n",
    "            continue\n",
    "        if i[1] <= j[1] and i[0] <= j[0] :\n",
    "            k = 0\n",
    "            break\n",
    "    if k == 1 :\n",
    "        result.append(i)\n",
    "\n",
    "\n",
    "pairs.mapPartitions(f).collect()\n",
    "\n",
    "# print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+-----------+----------+----------+------+--------------------+--------------------+--------------------+-------------------+--------------------+------------+--------------------+--------------------+\n",
      "|CustomerID|NameStyle|Title|  FirstName|MiddleName|  LastName|Suffix|         CompanyName|         SalesPerson|        EmailAddress|              Phone|        PasswordHash|PasswordSalt|             rowguid|        ModifiedDate|\n",
      "+----------+---------+-----+-----------+----------+----------+------+--------------------+--------------------+--------------------+-------------------+--------------------+------------+--------------------+--------------------+\n",
      "|         1|        0|  Mr.|    Orlando|        N.|       Gee|  NULL|        A Bike Store|adventure-works\\p...|orlando0@adventur...|       245-555-0173|L/Rlwxzp4w7RWmEgX...|    1KjXYs4=|3F5AE95E-B87D-4AE...|2005-08-01 00:00:...|\n",
      "|         2|        0|  Mr.|      Keith|      NULL|    Harris|  NULL|  Progressive Sports|adventure-works\\d...|keith0@adventure-...|       170-555-0127|YPdtRdvqeAhj6wyxE...|    fs1ZGhY=|E552F657-A9AF-4A7...|2006-08-01 00:00:...|\n",
      "|         3|        0|  Ms.|      Donna|        F.|  Carreras|  NULL|Advanced Bike Com...|adventure-works\\j...|donna0@adventure-...|       279-555-0130|LNoK27abGQo48gGue...|    YTNH5Rw=|130774B1-DB21-4EF...|2005-09-01 00:00:...|\n",
      "|         4|        0|  Ms.|      Janet|        M.|     Gates|  NULL|Modular Cycle Sys...|adventure-works\\j...|janet1@adventure-...|       710-555-0173|ElzTpSNbUW1Ut+L5c...|    nm7D5e4=|FF862851-1DAA-404...|2006-07-01 00:00:...|\n",
      "|         5|        0|  Mr.|       Lucy|      NULL|Harrington|  NULL|Metropolitan Spor...|adventure-works\\shu0|lucy0@adventure-w...|       828-555-0186|KJqV15wsX3PG8TS5G...|    cNFKU4w=|83905BDC-6F5E-4F7...|2006-09-01 00:00:...|\n",
      "|         6|        0|  Ms.|   Rosmarie|        J.|   Carroll|  NULL|Aerobic Exercise ...|adventure-works\\l...|rosmarie0@adventu...|       244-555-0112|OKT0scizCdIzymHHO...|    ihWf50M=|1A92DF88-BFA2-467...|2007-09-01 00:00:...|\n",
      "|         7|        0|  Mr.|    Dominic|        P.|      Gash|  NULL|    Associated Bikes|adventure-works\\shu0|dominic0@adventur...|       192-555-0173|ZccoP/jZGQm+Xpzc7...|    sPoUBSQ=|03E9273E-B193-448...|2006-07-01 00:00:...|\n",
      "|        10|        0|  Ms.|   Kathleen|        M.|     Garza|  NULL|Rural Cycle Emporium|adventure-works\\j...|kathleen0@adventu...|       150-555-0127|Qa3aMCxNbVLGrc0b9...|    Ls05W3g=|CDB6698D-2FF1-4FB...|2006-09-01 00:00:...|\n",
      "|        11|        0|  Ms.|  Katherine|      NULL|   Harding|  NULL|         Sharp Bikes|adventure-works\\j...|katherine0@advent...|       926-555-0159|uRlorVzDGNJIX9I+e...|    jpHKbqE=|750F3495-59C4-48A...|2005-08-01 00:00:...|\n",
      "|        12|        0|  Mr.|     Johnny|        A.|    Caprio|   Jr.|Bikes and Motorbikes|adventure-works\\g...|johnny0@adventure...|       112-555-0191|jtF9jBoFYeJTaET7x...|    wVLnvHo=|947BCAF1-1F32-44F...|2006-08-01 00:00:...|\n",
      "|        16|        0|  Mr.|Christopher|        R.|      Beck|   Jr.| Bulk Discount Store|adventure-works\\jae0|christopher1@adve...|1 (11) 500 555-0132|sKt9daCzEEKWAzivE...|    8KfYx/4=|C9381589-D31C-4EF...|2006-09-01 00:00:...|\n",
      "|        18|        0|  Mr.|      David|        J.|       Liu|  NULL|       Catalog Store|adventure-works\\m...|david20@adventure...|       440-555-0132|61zeTkO+eI5g8GG0s...|    c7Ttvv0=|C04D6B4D-94C6-4C5...|2005-08-01 00:00:...|\n",
      "|        19|        0|  Mr.|       John|        A.|    Beaver|  NULL|   Center Cycle Shop|adventure-works\\p...|john8@adventure-w...|       521-555-0195|DzbqWX7B3EK5Dub92...|    zXNgrJw=|69AE5D43-31BE-4B7...|2007-04-01 00:00:...|\n",
      "|        20|        0|  Ms.|       Jean|        P.|   Handley|  NULL|Central Discount ...|adventure-works\\d...|jean1@adventure-w...|       582-555-0113|o1GVo3vExeNzo0/ct...|    uMsvfdo=|E010C10A-F1C3-4BB...|2005-09-01 00:00:...|\n",
      "|        21|        0| NULL|    Jinghao|      NULL|       Liu|  NULL|Chic Department S...|adventure-works\\j...|jinghao1@adventur...|       928-555-0116|IaD5AeqK9mRiIrJi/...|    p6pOqKc=|564E0B42-4609-43D...|2006-09-01 00:00:...|\n",
      "|        22|        0|  Ms.|      Linda|        E.|   Burnett|  NULL|      Travel Systems|adventure-works\\j...|linda4@adventure-...|       121-555-0121|23AwhujCoXYSPiN/B...|    SmyIPjE=|9774AED6-D673-412...|2005-08-01 00:00:...|\n",
      "|        23|        0|  Mr.|      Kerim|      NULL|     Hanif|  NULL|          Bike World|adventure-works\\shu0|kerim0@adventure-...|       216-555-0122|d0WSjosAd7Y3XOWjN...|    33g5co8=|733F8250-3251-4C2...|2006-09-01 00:00:...|\n",
      "|        24|        0|  Mr.|      Kevin|      NULL|       Liu|  NULL|Eastside Departme...|adventure-works\\l...|kevin5@adventure-...|       926-555-0164|ylTpkIOHKLcjihNjS...|    TgZnUOg=|C111E51D-178D-4DB...|2006-09-01 00:00:...|\n",
      "|        25|        0|  Mr.|     Donald|        L.|   Blanton|  NULL|Coalition Bike Co...|adventure-works\\shu0|donald0@adventure...|       357-555-0161|pKYDelLBOZMO98GBz...|    jKtOaOw=|31D03546-FB2A-448...|2006-09-01 00:00:...|\n",
      "|        28|        0|  Ms.|     Jackie|        E.| Blackwell|  NULL|Commuter Bicycle ...|adventure-works\\j...|jackie0@adventure...|       972-555-0163|wqhgKfOTfef4Zo3cb...|    SZ+r60o=|9B8A04A4-D909-4F4...|2007-08-01 00:00:...|\n",
      "+----------+---------+-----+-----------+----------+----------+------+--------------------+--------------------+--------------------+-------------------+--------------------+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2018 Q1\n",
    "dfCustomer = spark.read.csv('Customer.csv', header=True, inferSchema=True)\n",
    "dfDetail = spark.read.csv('SalesOrderDetail.csv', header=True, inferSchema=True)\n",
    "dfHeader = spark.read.csv('SalesOrderHeader.csv', header=True, inferSchema=True)\n",
    "dfCustomer.show()\n",
    "##Not Finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5, 2, 7, 1, 5, 2, 7, 5]\n"
     ]
    }
   ],
   "source": [
    "#2018 Q3\n",
    "numPartitions = 10\n",
    "A = (0,0)\n",
    "B = (10000,0)\n",
    "C = (0,10000)\n",
    "\n",
    "points = sc.textFile('points.txt',numPartitions)\n",
    "pairs = points.map(lambda l: tuple(l.split()))\n",
    "pairs = pairs.map(lambda pair: (int(pair[0]),int(pair[1])))\n",
    "pairs.cache()\n",
    "\n",
    "def f(iterator):\n",
    "    counter = 0\n",
    "    for i in iterator:\n",
    "        if i[0] >= 0 and i[0] <=10000 and i[1] >= 0 and i[1] <= - i[0] + 10000:\n",
    "            counter += 1\n",
    "    yield counter\n",
    "\n",
    "temp = pairs.mapPartitions(f).collect()\n",
    "\n",
    "print(temp)\n",
    "# result = sum(temp)\n",
    "\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[], [], 0]]\n"
     ]
    }
   ],
   "source": [
    "array = [1, 2, 1, 1, 2, 2, 1, 1]\n",
    "s = 4\n",
    "\n",
    "from pyspark import SparkContext\n",
    "RDD = sc.parallelize(array, 2).cache()\n",
    "\n",
    "def sequential(nums):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    counter = 0\n",
    "    global s\n",
    "    while(j<=len(nums)):\n",
    "        if sum(nums[i:j]) == s:\n",
    "            counter += 1\n",
    "            i += 1\n",
    "        elif sum(nums[i:j]) < s:\n",
    "            j += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return counter\n",
    "\n",
    "def divide(it):\n",
    "    L = []\n",
    "    R = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    counter = 0\n",
    "    li = []\n",
    "    for i in it:\n",
    "        li.append(i)\n",
    "    global s\n",
    "    while(j<=len(li)):\n",
    "        if sum(li[i:j]) == s:\n",
    "            if i == 0:\n",
    "                L = li[i:j-1]\n",
    "            counter += 1\n",
    "            i += 1\n",
    "        elif sum(li[i:j]) < s:\n",
    "            if j == len(li):\n",
    "                R = li[i:j]\n",
    "            j += 1\n",
    "        else:\n",
    "            if i == 0:\n",
    "                L = li[i:j-1]\n",
    "            i += 1\n",
    "    return [[L,R,counter]]\n",
    "\n",
    "print(divide([1,1,1,1,1,1,1]))\n",
    "        \n",
    "\n",
    "def conquer(X):\n",
    "    counter = 0\n",
    "    for i in range(len(X)):\n",
    "        if i != 0 and i != len(X)-1:\n",
    "            counter += sequential(X[i][1] + X[i+1][0])\n",
    "        counter += X[i][2]\n",
    "    return counter\n",
    "        \n",
    "\n",
    "# X = RDD.mapPartitions(divide).collect()\n",
    "# print(X)\n",
    "# result = conquer(X)\n",
    "# print(result)\n",
    "\n",
    "# END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
